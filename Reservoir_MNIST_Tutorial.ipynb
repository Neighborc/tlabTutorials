{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Reservoir_MNIST_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neighborc/tlabTutorials/blob/master/Reservoir_MNIST_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sp1UmWDGJvTl",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B9S1Sp7OclP",
        "colab_type": "text"
      },
      "source": [
        "## Reservoir\n",
        "Here I give a walkthrough of how to use PyTorch for constructing a reservoir, essentially a randomized recurrent neural network, for classification. A more in-depth implementation which could be good for reference is [EchoTorch](https://github.com/nschaetti/EchoTorch)\n",
        "\n",
        "The main steps I use are to initialize a RNN with standard nodes, replace the weight matrix only the readout layer is trained. \n",
        "\n",
        "There are many other parameters to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AoAJfbfQaTgV",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as utils\n",
        "\n",
        "\n",
        "# set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_HVHg0tlaWyg"
      },
      "source": [
        "First in order to use PyTorch effectively we want to it up to use the availabe GPU device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xaIk5KTyaXZ0",
        "outputId": "3c29ee67-0a3b-4025-c15e-1b6a8e6ee2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setup the ability to run on a GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xn3jGPqTa5Qo"
      },
      "source": [
        "## Constructing the reservoir \n",
        "Here we create a recurrent neural network, set its internal weights according to the Echo State Property and then freeze all the weights except the output. \n",
        "Additionally aother simple way of doing this is by removing the final layer and running the data through the reservoir and then training a classifier on these node output values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dTLyeuYcaX0X",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, num_layers, num_classes, sequence_length, dropout=0, return_sequence=False):\n",
        "        super(RNN, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_layers = num_layers\n",
        "        self.return_sequence = return_sequence\n",
        "          \n",
        "        self.hidden = nn.RNN(input_size, n_hidden, num_layers, batch_first=True, bias=False)\n",
        "        self.drop_layer = nn.Dropout(p=dropout)\n",
        "        if return_sequence:\n",
        "          self.fc = nn.Linear(n_hidden*sequence_length, num_classes)\n",
        "        else:\n",
        "          self.fc = nn.Linear(n_hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.n_hidden).to(device) \n",
        "        \n",
        "        # Forward propagate\n",
        "        out, h_n = self.hidden(x, h0)  # out: tensor of shape (batch_size, seq_length, n_hidden)\n",
        "        if self.return_sequence:\n",
        "          out = out.contiguous().view(-1,self.n_hidden*self.sequence_length)\n",
        "          out = self.fc(out)\n",
        "        else:\n",
        "          out = self.fc(out[:,-1,:])\n",
        "        return out\n",
        "\n",
        "def create_reservoir_matrix(size=(10,10), spectral_radius=0.9, sparsity=0.5):\n",
        "    \"\"\" Creates the W_res of the ESN which has the ESP\n",
        "    inputs: \n",
        "    size: square matrix representing the size of teh reservoir connections\n",
        "    spectral_radius: largest eigenvalue in reservoir matrix should be <1\n",
        "    sparsity: connectivity of matrix, 1.0 indicates full connection \n",
        "    \"\"\"\n",
        "    # generate uniformly random from U[0,1] *2 -1 makes it U[-1,1]\n",
        "    W_res = torch.rand(size)*2-1\n",
        "\n",
        "    # create a sparse mask array then multiply reservoir weights by sparse mask\n",
        "    # sparse mask is done by generating uniform[0,1] then setting True <=sparsity\n",
        "    W_res = W_res*(torch.rand_like(W_res) <= sparsity).type(W_res.dtype)\n",
        "    \n",
        "    # scale the matrix to have the desired spectral radius\n",
        "    W_res = W_res*spectral_radius/(np.max(np.abs(np.linalg.eigvals(W_res))))\n",
        "\n",
        "    return W_res\n",
        "\n",
        "\n",
        "from numpy import linalg as LA\n",
        "from scipy.sparse import random as sparserand\n",
        "def create_sparse_connection_matrix(size=(10,10),spectral_radius=0.9, sparsity=0.5):\n",
        "  \"\"\" Creates a sparse matrix using a different library which is not probabilistic\n",
        "  and so should have a better consistancy for small networks \n",
        "  returns a numpy array\"\"\"\n",
        "  matrix = sparserand(size[0],size[1], density=sparsity)\n",
        "  matrix = matrix.A*2-1 # shift from U[0,1] to U[-1,1]\n",
        "\n",
        "  max_eigval = max(abs(LA.eigvals(matrix.A)))\n",
        "  if max_eigval == 0:\n",
        "    # checks for an entirely sparse matrix\n",
        "    return matrix.A\n",
        "\n",
        "  else:\n",
        "    matrix = spectral_radius*matrix.A/max_eigval\n",
        "    return matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGs9Y3uWnY28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axtSmtH8piXg",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Example\n",
        "\n",
        "Load in the MNIST data, transform it, and build some batched dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnzbsEo3piXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8yeE9xJpiXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Hyper Parameters \n",
        "input_size = (28, 28)\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1024\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eixF46gUpiXn",
        "colab_type": "code",
        "outputId": "d50d0451-8066-485e-f827-18babfbcd758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', train=True, \n",
        "                            transform=transforms.Compose([\n",
        "                            transforms.Resize(input_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))]),\n",
        "                            download=True)\n",
        "test_dataset = dsets.MNIST('./data', train=False,transform=transforms.Compose([\n",
        "                            transforms.Resize(input_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))]), download=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8520095.84it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 128866.50it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2014664.75it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 49779.77it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF1n5R8mpiXp",
        "colab_type": "code",
        "outputId": "9ee83754-2c64-43e5-c51a-d833c109e805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# check the imported dataset shape\n",
        "print(train_dataset[0][0].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udPohKZzpiXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset loaders (handle mini-batching of data)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True) \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--JjV7hGpiXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_accuracy(loader, total_size, split='train'):\n",
        "    \"\"\"Measure the accuracy for the entire dataset\"\"\" \n",
        "    \n",
        "    #set PyTorch model in eval mode for faster inference, no grads passed\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sequences, labels in loader:\n",
        "        sequences = sequences.view(-1,28,28)\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(sequences) \n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        total += labels.size(0) \n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print(f'Accuracy of the model on the {total_size} {split} sequences: {float(correct) / total:3.1%}')\n",
        "    \n",
        "    return float(correct) / total    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m29357lWq_CY",
        "colab_type": "text"
      },
      "source": [
        "At this point we can go through the steps to make the Echo State Network. \n",
        "1. Initialize an RNN\n",
        "2. Create the $W_{res}$ we want for our ESN in the shape of the RNN\n",
        "3. Replace the hidden weights with $W_{res}$\n",
        "4. Set the requires_grad to False for the hidden layer, this freezes the layer weights in place and doesn't pass any gradient to the layers before this one as well\n",
        "\n",
        "You could make the class automatically construct the reservoir as well if you like, using potentially a classmethod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YbTPwlwpiX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Hyper-parameters\n",
        "n_hidden = 50\n",
        "num_layers = 1 # only works with 1 for now \n",
        "batch_size = batch_size_train\n",
        "learning_rate = 0.001\n",
        "dropout = 0.05\n",
        "\n",
        "# create the internal weight matrix\n",
        "W_res = create_reservoir_matrix(size=(n_hidden, n_hidden), spectral_radius=0.9, sparsity=1.0)\n",
        "\n",
        "model = RNN(input_size, n_hidden, num_layers, num_classes, \n",
        "                  sequence_length=sequence_length, dropout=dropout, return_sequence=True)\n",
        "\n",
        "# set the internal hidden weight matrix as the reservoir values \n",
        "model.hidden.weight_hh_l0 = nn.Parameter(W_res, requires_grad=False)\n",
        "input_scale = 0.5\n",
        "model.hidden.weight_ih_l0 = nn.Parameter((torch.rand((n_hidden,input_size))*2-1)*input_scale, requires_grad=False)\n",
        "\n",
        "# move to the GPU\n",
        "model.to(device)\n",
        "\n",
        "# Freeze all hidden layers so no gradient update occurs\n",
        "for param in model.hidden.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# only need the gradient for the fully connected layer, weight_decay adds l2 norm\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "# CrossEntropyLoss takes care of the Softmax evaluation \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAoTQ0PpiXX",
        "colab_type": "text"
      },
      "source": [
        "### Train the model \n",
        "Here we train the model and keep track of the train and test losses. Note if you rerun this cell the model will continue to train from it's previous state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9nha07xpiX3",
        "colab_type": "text"
      },
      "source": [
        "Note this can take around 5 minutes on a CPU, much faster if you run it on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70iJntlhpiX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8017e95d-99fa-460e-8197-640d292b1666"
      },
      "source": [
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "num_epochs = 15\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (sequences, labels) in enumerate(train_loader):\n",
        "        # reshape to (batch_size, time_step, input size)\n",
        "        sequences = sequences.view(-1,28,28)\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(sequences) \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print out your accuracy along the way\n",
        "        if (i + 1) % 70 == 0:\n",
        "            print(f'Epoch: [{epoch+1}/{num_epochs}], Step: [{i+1}/{len(train_dataset)//batch_size}], Loss: {loss.item():3}') \n",
        "    \n",
        "    \n",
        "    # records accuracy on first epoch and on the 5th ones after that\n",
        "    if epoch==0 or (epoch+1)%5==0:\n",
        "        test_acc_epoch = epoch_accuracy(test_loader, len(test_dataset), 'test')\n",
        "        test_acc.append(test_acc_epoch)\n",
        "        train_acc_epoch = epoch_accuracy(train_loader, len(train_dataset), 'train')\n",
        "        train_acc.append(train_acc_epoch)\n",
        "\n",
        "plt.plot(train_acc)\n",
        "plt.plot(test_acc)\n",
        "plt.legend(['train_acc','test_acc'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [1/15], Step: [70/937], Loss: 0.38344958424568176\n",
            "Epoch: [1/15], Step: [140/937], Loss: 0.4095509648323059\n",
            "Epoch: [1/15], Step: [210/937], Loss: 0.5083675384521484\n",
            "Epoch: [1/15], Step: [280/937], Loss: 0.22053581476211548\n",
            "Epoch: [1/15], Step: [350/937], Loss: 0.3118206262588501\n",
            "Epoch: [1/15], Step: [420/937], Loss: 0.337104856967926\n",
            "Epoch: [1/15], Step: [490/937], Loss: 0.29958271980285645\n",
            "Epoch: [1/15], Step: [560/937], Loss: 0.28666114807128906\n",
            "Epoch: [1/15], Step: [630/937], Loss: 0.14635005593299866\n",
            "Epoch: [1/15], Step: [700/937], Loss: 0.11091510206460953\n",
            "Epoch: [1/15], Step: [770/937], Loss: 0.31741881370544434\n",
            "Epoch: [1/15], Step: [840/937], Loss: 0.16884376108646393\n",
            "Epoch: [1/15], Step: [910/937], Loss: 0.24079939723014832\n",
            "Accuracy of the model on the 10000 test sequences: 93.1%\n",
            "Accuracy of the model on the 60000 train sequences: 93.2%\n",
            "Epoch: [2/15], Step: [70/937], Loss: 0.3867510259151459\n",
            "Epoch: [2/15], Step: [140/937], Loss: 0.3512437045574188\n",
            "Epoch: [2/15], Step: [210/937], Loss: 0.3881421685218811\n",
            "Epoch: [2/15], Step: [280/937], Loss: 0.2151952087879181\n",
            "Epoch: [2/15], Step: [350/937], Loss: 0.45397239923477173\n",
            "Epoch: [2/15], Step: [420/937], Loss: 0.18265725672245026\n",
            "Epoch: [2/15], Step: [490/937], Loss: 0.15772977471351624\n",
            "Epoch: [2/15], Step: [560/937], Loss: 0.35681912302970886\n",
            "Epoch: [2/15], Step: [630/937], Loss: 0.32882100343704224\n",
            "Epoch: [2/15], Step: [700/937], Loss: 0.14790809154510498\n",
            "Epoch: [2/15], Step: [770/937], Loss: 0.334916889667511\n",
            "Epoch: [2/15], Step: [840/937], Loss: 0.19970837235450745\n",
            "Epoch: [2/15], Step: [910/937], Loss: 0.2910795509815216\n",
            "Epoch: [3/15], Step: [70/937], Loss: 0.19001030921936035\n",
            "Epoch: [3/15], Step: [140/937], Loss: 0.10753233730792999\n",
            "Epoch: [3/15], Step: [210/937], Loss: 0.19708378612995148\n",
            "Epoch: [3/15], Step: [280/937], Loss: 0.1377207338809967\n",
            "Epoch: [3/15], Step: [350/937], Loss: 0.2574315667152405\n",
            "Epoch: [3/15], Step: [420/937], Loss: 0.20170801877975464\n",
            "Epoch: [3/15], Step: [490/937], Loss: 0.18318845331668854\n",
            "Epoch: [3/15], Step: [560/937], Loss: 0.23052817583084106\n",
            "Epoch: [3/15], Step: [630/937], Loss: 0.2786712050437927\n",
            "Epoch: [3/15], Step: [700/937], Loss: 0.25259748101234436\n",
            "Epoch: [3/15], Step: [770/937], Loss: 0.18786481022834778\n",
            "Epoch: [3/15], Step: [840/937], Loss: 0.16077813506126404\n",
            "Epoch: [3/15], Step: [910/937], Loss: 0.1202581599354744\n",
            "Epoch: [4/15], Step: [70/937], Loss: 0.18528150022029877\n",
            "Epoch: [4/15], Step: [140/937], Loss: 0.25136157870292664\n",
            "Epoch: [4/15], Step: [210/937], Loss: 0.20010317862033844\n",
            "Epoch: [4/15], Step: [280/937], Loss: 0.2638649344444275\n",
            "Epoch: [4/15], Step: [350/937], Loss: 0.20592395961284637\n",
            "Epoch: [4/15], Step: [420/937], Loss: 0.13156157732009888\n",
            "Epoch: [4/15], Step: [490/937], Loss: 0.08540557324886322\n",
            "Epoch: [4/15], Step: [560/937], Loss: 0.2141195684671402\n",
            "Epoch: [4/15], Step: [630/937], Loss: 0.43354809284210205\n",
            "Epoch: [4/15], Step: [700/937], Loss: 0.2447313517332077\n",
            "Epoch: [4/15], Step: [770/937], Loss: 0.25616100430488586\n",
            "Epoch: [4/15], Step: [840/937], Loss: 0.22100193798542023\n",
            "Epoch: [4/15], Step: [910/937], Loss: 0.19730675220489502\n",
            "Epoch: [5/15], Step: [70/937], Loss: 0.09074743092060089\n",
            "Epoch: [5/15], Step: [140/937], Loss: 0.06012926995754242\n",
            "Epoch: [5/15], Step: [210/937], Loss: 0.1685093343257904\n",
            "Epoch: [5/15], Step: [280/937], Loss: 0.14136993885040283\n",
            "Epoch: [5/15], Step: [350/937], Loss: 0.24623069167137146\n",
            "Epoch: [5/15], Step: [420/937], Loss: 0.13454437255859375\n",
            "Epoch: [5/15], Step: [490/937], Loss: 0.401907742023468\n",
            "Epoch: [5/15], Step: [560/937], Loss: 0.09244973212480545\n",
            "Epoch: [5/15], Step: [630/937], Loss: 0.0849180668592453\n",
            "Epoch: [5/15], Step: [700/937], Loss: 0.10981258749961853\n",
            "Epoch: [5/15], Step: [770/937], Loss: 0.24807108938694\n",
            "Epoch: [5/15], Step: [840/937], Loss: 0.18438106775283813\n",
            "Epoch: [5/15], Step: [910/937], Loss: 0.31628936529159546\n",
            "Accuracy of the model on the 10000 test sequences: 94.5%\n",
            "Accuracy of the model on the 60000 train sequences: 95.4%\n",
            "Epoch: [6/15], Step: [70/937], Loss: 0.3308540880680084\n",
            "Epoch: [6/15], Step: [140/937], Loss: 0.12322970479726791\n",
            "Epoch: [6/15], Step: [210/937], Loss: 0.22688452899456024\n",
            "Epoch: [6/15], Step: [280/937], Loss: 0.20851948857307434\n",
            "Epoch: [6/15], Step: [350/937], Loss: 0.21277202665805817\n",
            "Epoch: [6/15], Step: [420/937], Loss: 0.24989984929561615\n",
            "Epoch: [6/15], Step: [490/937], Loss: 0.14811298251152039\n",
            "Epoch: [6/15], Step: [560/937], Loss: 0.12249504774808884\n",
            "Epoch: [6/15], Step: [630/937], Loss: 0.12243730574846268\n",
            "Epoch: [6/15], Step: [700/937], Loss: 0.11403703689575195\n",
            "Epoch: [6/15], Step: [770/937], Loss: 0.0786563903093338\n",
            "Epoch: [6/15], Step: [840/937], Loss: 0.24228984117507935\n",
            "Epoch: [6/15], Step: [910/937], Loss: 0.1930255889892578\n",
            "Epoch: [7/15], Step: [70/937], Loss: 0.23030290007591248\n",
            "Epoch: [7/15], Step: [140/937], Loss: 0.07467712461948395\n",
            "Epoch: [7/15], Step: [210/937], Loss: 0.0938434824347496\n",
            "Epoch: [7/15], Step: [280/937], Loss: 0.13578253984451294\n",
            "Epoch: [7/15], Step: [350/937], Loss: 0.112298883497715\n",
            "Epoch: [7/15], Step: [420/937], Loss: 0.08253566920757294\n",
            "Epoch: [7/15], Step: [490/937], Loss: 0.13442134857177734\n",
            "Epoch: [7/15], Step: [560/937], Loss: 0.28606587648391724\n",
            "Epoch: [7/15], Step: [630/937], Loss: 0.1702963411808014\n",
            "Epoch: [7/15], Step: [700/937], Loss: 0.05147577077150345\n",
            "Epoch: [7/15], Step: [770/937], Loss: 0.23105423152446747\n",
            "Epoch: [7/15], Step: [840/937], Loss: 0.13518892228603363\n",
            "Epoch: [7/15], Step: [910/937], Loss: 0.17465829849243164\n",
            "Epoch: [8/15], Step: [70/937], Loss: 0.14025261998176575\n",
            "Epoch: [8/15], Step: [140/937], Loss: 0.06877100467681885\n",
            "Epoch: [8/15], Step: [210/937], Loss: 0.15114183723926544\n",
            "Epoch: [8/15], Step: [280/937], Loss: 0.16616080701351166\n",
            "Epoch: [8/15], Step: [350/937], Loss: 0.19637802243232727\n",
            "Epoch: [8/15], Step: [420/937], Loss: 0.191216841340065\n",
            "Epoch: [8/15], Step: [490/937], Loss: 0.1803550273180008\n",
            "Epoch: [8/15], Step: [560/937], Loss: 0.1600339710712433\n",
            "Epoch: [8/15], Step: [630/937], Loss: 0.1341754049062729\n",
            "Epoch: [8/15], Step: [700/937], Loss: 0.1376037299633026\n",
            "Epoch: [8/15], Step: [770/937], Loss: 0.15770238637924194\n",
            "Epoch: [8/15], Step: [840/937], Loss: 0.03270741552114487\n",
            "Epoch: [8/15], Step: [910/937], Loss: 0.11000844836235046\n",
            "Epoch: [9/15], Step: [70/937], Loss: 0.06867703050374985\n",
            "Epoch: [9/15], Step: [140/937], Loss: 0.14070378243923187\n",
            "Epoch: [9/15], Step: [210/937], Loss: 0.06015382707118988\n",
            "Epoch: [9/15], Step: [280/937], Loss: 0.14467258751392365\n",
            "Epoch: [9/15], Step: [350/937], Loss: 0.1556127518415451\n",
            "Epoch: [9/15], Step: [420/937], Loss: 0.17540213465690613\n",
            "Epoch: [9/15], Step: [490/937], Loss: 0.10125361382961273\n",
            "Epoch: [9/15], Step: [560/937], Loss: 0.15735261142253876\n",
            "Epoch: [9/15], Step: [630/937], Loss: 0.05016528069972992\n",
            "Epoch: [9/15], Step: [700/937], Loss: 0.1489505022764206\n",
            "Epoch: [9/15], Step: [770/937], Loss: 0.06303450465202332\n",
            "Epoch: [9/15], Step: [840/937], Loss: 0.15141816437244415\n",
            "Epoch: [9/15], Step: [910/937], Loss: 0.07729318737983704\n",
            "Epoch: [10/15], Step: [70/937], Loss: 0.10234266519546509\n",
            "Epoch: [10/15], Step: [140/937], Loss: 0.08632685244083405\n",
            "Epoch: [10/15], Step: [210/937], Loss: 0.1693168580532074\n",
            "Epoch: [10/15], Step: [280/937], Loss: 0.10576943308115005\n",
            "Epoch: [10/15], Step: [350/937], Loss: 0.2068658322095871\n",
            "Epoch: [10/15], Step: [420/937], Loss: 0.2535654604434967\n",
            "Epoch: [10/15], Step: [490/937], Loss: 0.11850836127996445\n",
            "Epoch: [10/15], Step: [560/937], Loss: 0.3150460124015808\n",
            "Epoch: [10/15], Step: [630/937], Loss: 0.06023988872766495\n",
            "Epoch: [10/15], Step: [700/937], Loss: 0.05798352509737015\n",
            "Epoch: [10/15], Step: [770/937], Loss: 0.17516711354255676\n",
            "Epoch: [10/15], Step: [840/937], Loss: 0.13039246201515198\n",
            "Epoch: [10/15], Step: [910/937], Loss: 0.05910512059926987\n",
            "Accuracy of the model on the 10000 test sequences: 94.4%\n",
            "Accuracy of the model on the 60000 train sequences: 95.9%\n",
            "Epoch: [11/15], Step: [70/937], Loss: 0.06535230576992035\n",
            "Epoch: [11/15], Step: [140/937], Loss: 0.15629678964614868\n",
            "Epoch: [11/15], Step: [210/937], Loss: 0.1459541767835617\n",
            "Epoch: [11/15], Step: [280/937], Loss: 0.1437114179134369\n",
            "Epoch: [11/15], Step: [350/937], Loss: 0.09274527430534363\n",
            "Epoch: [11/15], Step: [420/937], Loss: 0.09322918951511383\n",
            "Epoch: [11/15], Step: [490/937], Loss: 0.08866243809461594\n",
            "Epoch: [11/15], Step: [560/937], Loss: 0.15715055167675018\n",
            "Epoch: [11/15], Step: [630/937], Loss: 0.3316181004047394\n",
            "Epoch: [11/15], Step: [700/937], Loss: 0.10425220429897308\n",
            "Epoch: [11/15], Step: [770/937], Loss: 0.21749980747699738\n",
            "Epoch: [11/15], Step: [840/937], Loss: 0.05584806203842163\n",
            "Epoch: [11/15], Step: [910/937], Loss: 0.09583988040685654\n",
            "Epoch: [12/15], Step: [70/937], Loss: 0.04222031682729721\n",
            "Epoch: [12/15], Step: [140/937], Loss: 0.059741176664829254\n",
            "Epoch: [12/15], Step: [210/937], Loss: 0.19743657112121582\n",
            "Epoch: [12/15], Step: [280/937], Loss: 0.07713678479194641\n",
            "Epoch: [12/15], Step: [350/937], Loss: 0.06546644866466522\n",
            "Epoch: [12/15], Step: [420/937], Loss: 0.2165786623954773\n",
            "Epoch: [12/15], Step: [490/937], Loss: 0.1645173728466034\n",
            "Epoch: [12/15], Step: [560/937], Loss: 0.20980516076087952\n",
            "Epoch: [12/15], Step: [630/937], Loss: 0.3031380772590637\n",
            "Epoch: [12/15], Step: [700/937], Loss: 0.09185728430747986\n",
            "Epoch: [12/15], Step: [770/937], Loss: 0.058217741549015045\n",
            "Epoch: [12/15], Step: [840/937], Loss: 0.18174399435520172\n",
            "Epoch: [12/15], Step: [910/937], Loss: 0.15099938213825226\n",
            "Epoch: [13/15], Step: [70/937], Loss: 0.07492057979106903\n",
            "Epoch: [13/15], Step: [140/937], Loss: 0.12753987312316895\n",
            "Epoch: [13/15], Step: [210/937], Loss: 0.10977168381214142\n",
            "Epoch: [13/15], Step: [280/937], Loss: 0.0963953360915184\n",
            "Epoch: [13/15], Step: [350/937], Loss: 0.18128131330013275\n",
            "Epoch: [13/15], Step: [420/937], Loss: 0.1030251681804657\n",
            "Epoch: [13/15], Step: [490/937], Loss: 0.10845999419689178\n",
            "Epoch: [13/15], Step: [560/937], Loss: 0.2271314114332199\n",
            "Epoch: [13/15], Step: [630/937], Loss: 0.06046414002776146\n",
            "Epoch: [13/15], Step: [700/937], Loss: 0.019638188183307648\n",
            "Epoch: [13/15], Step: [770/937], Loss: 0.12929099798202515\n",
            "Epoch: [13/15], Step: [840/937], Loss: 0.05070730298757553\n",
            "Epoch: [13/15], Step: [910/937], Loss: 0.18588830530643463\n",
            "Epoch: [14/15], Step: [70/937], Loss: 0.2295989990234375\n",
            "Epoch: [14/15], Step: [140/937], Loss: 0.05921909958124161\n",
            "Epoch: [14/15], Step: [210/937], Loss: 0.057749755680561066\n",
            "Epoch: [14/15], Step: [280/937], Loss: 0.08492307364940643\n",
            "Epoch: [14/15], Step: [350/937], Loss: 0.021415360271930695\n",
            "Epoch: [14/15], Step: [420/937], Loss: 0.21823418140411377\n",
            "Epoch: [14/15], Step: [490/937], Loss: 0.25467488169670105\n",
            "Epoch: [14/15], Step: [560/937], Loss: 0.17198169231414795\n",
            "Epoch: [14/15], Step: [630/937], Loss: 0.15340113639831543\n",
            "Epoch: [14/15], Step: [700/937], Loss: 0.0870286226272583\n",
            "Epoch: [14/15], Step: [770/937], Loss: 0.06794484704732895\n",
            "Epoch: [14/15], Step: [840/937], Loss: 0.19256407022476196\n",
            "Epoch: [14/15], Step: [910/937], Loss: 0.30152612924575806\n",
            "Epoch: [15/15], Step: [70/937], Loss: 0.14138104021549225\n",
            "Epoch: [15/15], Step: [140/937], Loss: 0.05643612891435623\n",
            "Epoch: [15/15], Step: [210/937], Loss: 0.06490489840507507\n",
            "Epoch: [15/15], Step: [280/937], Loss: 0.07345195859670639\n",
            "Epoch: [15/15], Step: [350/937], Loss: 0.14209459722042084\n",
            "Epoch: [15/15], Step: [420/937], Loss: 0.05611681938171387\n",
            "Epoch: [15/15], Step: [490/937], Loss: 0.04816511273384094\n",
            "Epoch: [15/15], Step: [560/937], Loss: 0.19037196040153503\n",
            "Epoch: [15/15], Step: [630/937], Loss: 0.054661527276039124\n",
            "Epoch: [15/15], Step: [700/937], Loss: 0.03825753927230835\n",
            "Epoch: [15/15], Step: [770/937], Loss: 0.07060711830854416\n",
            "Epoch: [15/15], Step: [840/937], Loss: 0.10618967562913895\n",
            "Epoch: [15/15], Step: [910/937], Loss: 0.07760588824748993\n",
            "Accuracy of the model on the 10000 test sequences: 95.0%\n",
            "Accuracy of the model on the 60000 train sequences: 96.8%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa78af22f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD7CAYAAABjVUMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXhU1fnA8e9LCEvYSQCBsASCCMge\nNhFB3FCrKKh1Q0EFEbR20Yqt2roV29L2py2ioICgFS1uVLEoEERBJWFfZElYJGELCQECWSfv7497\nwSEEMpBJZpJ5P8+Tx8m59545J4PnnXvuve8RVcUYY0zoqRLoBhhjjAkMCwDGGBOiLAAYY0yIsgBg\njDEhygKAMcaEKAsAxhgTonwKACIyRES2iEiSiEwoZnsrEVkkIutEZImIRLvll4vIGq+fHBG5yd0W\nIyLfu3W+JyLV/Ns1Y4wxZyMlPQcgImHAVuAqIAVIAO5Q1U1e+/wH+FRV3xKRwcAoVR1RpJ6GQBIQ\nrarHReR94ENVnSMirwFrVXWKPztnjDHmzHwJAP2AP6rqNe7vTwKo6kSvfTYCQ1R1t4gIcFhV6xap\nZwwwUFXvcvdJAy5Q1YKi73EmUVFR2rp163PupDHGhLKVK1ceVNVGRcur+nBsc2C31+8pQJ8i+6wF\nhgEvAzcDdUQkUlXTvfa5Hfi7+zoSyFTVAq86m5fUkNatW5OYmOhDk40xxpwgIruKK/fXReDHgIEi\nshoYCKQCHq83bwp0Bhaca8UiMkZEEkUkMS0tzU/NNcYY40sASAVaeP0e7ZadpKp7VHWYqnYHfu+W\nZXrtchvwkarmu7+nA/VF5MQZyGl1etU9VVXjVDWuUaPTzmCMMcacJ18CQALQzr1rpxrOVM487x1E\nJEpETtT1JDC9SB13AO+e+EWdCw/xwC1u0b3AJ+fefGOMMeerxGsA7kXah3Gmb8KA6aq6UUSeAxJV\ndR4wCJgoIgosBcafOF5EWuOcQXxVpOongDki8gKwGnjzfDqQn59PSkoKOTk553N4yKtRowbR0dGE\nh4cHuinGmHJW4l1AwSQuLk6LXgTesWMHderUITIyEufmIuMrVSU9PZ2jR48SExMT6OYYY8qIiKxU\n1bii5RX+SeCcnBwb/M+TiBAZGWlnT8aEqAofAAAb/EvB/nbGhK5KEQCMMaay2rr/KM/+dyMFnkK/\n1+3Lg2DGGGPK2drdmby6JIkFG/dTMzyMYd2j6Rxdz6/vYWcApZSZmcmrr756zsddd911ZGZmlryj\nMSZkqCrfJqcz4s3vGTp5Gd8mp/OLwbEsmzDY74M/2BlAqZ0IAOPGjTulvKCggKpVz/znnT9/flk3\nzRhTQagq8VsOMDk+mZW7DhFVuzoTrr2Iu/q0pE6NsrtFu1IFgGf/u5FNe474tc6Ozeryhxs6nXH7\nhAkTSE5Oplu3boSHh1OjRg0aNGjA5s2b2bp1KzfddBO7d+8mJyeHRx99lDFjxgA/5TXKysri2muv\n5dJLL2X58uU0b96cTz75hJo1axb7ftOmTWPq1Knk5eURGxvL7NmziYiIYP/+/YwdO5bt27cDMGXK\nFC655BJmzZrFpEmTEBG6dOnC7Nmz/fr3McacP0+hMn/9Xl5dkswPe4/QvH5Nnh/aiVvjWlAjPKzM\n379SBYBAeOmll9iwYQNr1qxhyZIlXH/99WzYsOHkffXTp0+nYcOGZGdn06tXL4YPH05kZOQpdWzb\nto13332XadOmcdttt/HBBx9w9913F/t+w4YNY/To0QA89dRTvPnmmzzyyCP84he/YODAgXz00Ud4\nPB6ysrLYuHEjL7zwAsuXLycqKoqMjIyy/WMYY3ySV1DIx6tTmfJVMjsOHqNNo1pMurUrQ7s1Izys\n/GbmK1UAONs39fLSu3fvUx6qeuWVV/joo48A2L17N9u2bTstAMTExNCtWzcAevbsyc6dO89Y/4YN\nG3jqqafIzMwkKyuLa65xMmgvXryYWbNmARAWFka9evWYNWsWt956K1FRUQA0bNjQb/00xpy77DwP\ncxJ+ZOrS7ew9nEOnZnWZclcPru50AWFVyv+W7EoVAIJBrVq1Tr5esmQJCxcu5NtvvyUiIoJBgwYV\n+9BV9erVT74OCwsjOzv7jPWPHDmSjz/+mK5duzJz5kyWLFni1/YbY/zvSE4+s7/dxfRvdpB+LI/e\nrRsycVhnBl7YKKDP4thdQKVUp04djh49Wuy2w4cP06BBAyIiIti8eTPfffddqd/v6NGjNG3alPz8\nfN55552T5VdccQVTpjgLqnk8Hg4fPszgwYP5z3/+Q3q6syyDTQEZU77Ss3L564LN9J+4mL8u2MLF\nzevx/oP9eH9sPwa1bxzwBzHtDKCUIiMj6d+/PxdffDE1a9akSZMmJ7cNGTKE1157jQ4dOtC+fXv6\n9u1b6vd7/vnn6dOnD40aNaJPnz4ng8/LL7/MmDFjePPNNwkLC2PKlCn069eP3//+9wwcOJCwsDC6\nd+/OzJkzS90GY8zZ7cnMZtrX23l3xY/kFhRy7cUXMG5QLBc39/+tnKVR4ZPB/fDDD3To0CFALaoc\n7G9ojH/sOHiM15Yk8+HqFFThpu7NGTuwLbGNawe0XWdKBmdnAMYYU0o/7D3Cq0uS+WzdHqqGVeGO\n3i0Zc1kbohtEBLppZ2UBIEiNHz+eZcuWnVL26KOPMmrUqAC1yBhT1Mpdh3g1PolFmw9Qu3pVxlzW\nlvsubU3jOjUC3TSfWAAIUpMnTw50E4wxxVBVliWl86/4bXy3PYP6EeH8+qoLubdfa+pFVKyFlSwA\nGGOMDwoLlS9/2M+r8UmsTTlMk7rVeer6DtzRuyW1qlfMobRittoYY8pJgaeQT9ft5dUlSWzdn0XL\nhhFMHNaZYT2aU71q2adrKEs+BQARGQK8jLMm8Buq+lKR7a1wFoJvBGQAd6tqirutJfAGzrrAClyn\nqjtFZCYwEDjsVjNSVdeUukfGGOMHOfkePliVwmtfJbM7I5sLm9Tm5du7cX3nplQtx3QNZanEXohI\nGDAZuBboCNwhIh2L7DYJmKWqXYDngIle22YBf1XVDkBv4IDXtsdVtZv7UyEH//NNBw3wf//3fxw/\nftzPLTLGlMax3ALe+Ho7l/0lnt9/tIGGtaoz7Z44/vfoZQzt1rzSDP7g25PAvYEkVd2uqnnAHGBo\nkX06Aovd1/EntruBoqqqfgmgqlmqWqlGPAsAxlQOmcfzeHnhNvr/eTEvfPYDsY1r884Dffh43CVc\n1bEJVQKQq6es+TIF1BzY7fV7CtCnyD5rgWE400Q3A3VEJBK4EMgUkQ+BGGAhMEFVPe5xL4rIM8Ai\ntzy36JuLyBhgDEDLli197Ve58U4HfdVVV9G4cWPef/99cnNzufnmm3n22Wc5duwYt912GykpKXg8\nHp5++mn279/Pnj17uPzyy4mKiiI+Pr7Y+h966CESEhLIzs7mlltu4dlnnwUgISGBRx99lGPHjlG9\nenUWLVpEREQETzzxBP/73/+oUqUKo0eP5pFHHinPP4cxFc6Bozm8+fUO3v5uF8fyPFzZoTHjLo+l\nR8sGgW5amfPXReDHgH+JyEhgKZAKeNz6BwDdgR+B94CRwJvAk8A+oBowFXgCZ/roFKo61d1OXFzc\n2R9b/nwC7Fvvh+54uaAzXPvSGTd7p4P+4osvmDt3LitWrEBVufHGG1m6dClpaWk0a9aMzz77DHBy\nBNWrV4+///3vxMfHn8zWWZwXX3yRhg0b4vF4uOKKK1i3bh0XXXQRP//5z3nvvffo1asXR44coWbN\nmkydOpWdO3eyZs0aqlatarl/jDmL3RnHmbp0O+8l7qbAU8jPujTjoUFt6dC0bqCbVm58CQCpOBdw\nT4h2y05S1T04ZwCISG1guKpmikgKsEZVt7vbPgb6Am+q6l738FwRmYETRCq0L774gi+++ILu3bsD\nkJWVxbZt2xgwYAC/+c1veOKJJ/jZz37GgAEDfK7z/fffZ+rUqRQUFLB37142bdqEiNC0aVN69eoF\nQN26zj/YhQsXMnbs2JMrkVn6Z2NOl3TgKK8uSeaTNXuoInBLz2gevKwtraNqlXxwJeNLAEgA2olI\nDM7Afztwp/cOIhIFZKhqIc43++lex9YXkUaqmgYMBhLdY5qq6l5x0uHdBGwodW/O8k29PKgqTz75\nJA8++OBp21atWsX8+fN56qmnuOKKK3jmmWdKrG/Hjh1MmjSJhIQEGjRowMiRI4tNJ22MKdn6lMO8\nuiSJ/23cR/WqVbi3X2tGXxZD03rFr74XCkq8CKyqBcDDwALgB+B9Vd0oIs+JyI3uboOALSKyFWgC\nvOge68H5Zr9IRNYDAkxzj3nHLVsPRAEv+K1X5cg7HfQ111zD9OnTycrKAiA1NZUDBw6wZ88eIiIi\nuPvuu3n88cdZtWrVaccW58iRI9SqVYt69eqxf/9+Pv/8cwDat2/P3r17SUhIAJwU0QUFBVx11VW8\n/vrrFBQUAJb+2RiAFTsyuGf6Cm741zd8k3SQhy+PZdkTg3nmho4hPfiDj9cAVHU+ML9I2TNer+cC\nc89w7JdAl2LKB59TS4OUdzroa6+9ljvvvJN+/foBULt2bd5++22SkpJ4/PHHqVKlCuHh4Sfz9o8Z\nM4YhQ4bQrFmzYi8Cd+3ale7du3PRRRfRokUL+vfvD0C1atV47733eOSRR8jOzqZmzZosXLiQBx54\ngK1bt9KlSxfCw8MZPXo0Dz/8cPn9MYwJEqrKkq1pvBqfRMLOQ0TWqsZvh7Tn7r6tqFuGi6xXNJYO\n2tjf0FQankJlwcZ9TI5PYuOeIzSrV4MHB7bltrgW1KxWsZ/aLQ1LB22MqbTyPT8tsr497Rhtomrx\nl1u6cFO35lSrWnke3PI3CwBBok+fPuTmnvoYxOzZs+ncuXOAWmRM8MvJ9/Bewm6mLt1OamY2HZvW\nZfKdPRhycWAWWa9oLAAEie+//z7QTTCmwjiak8/b3/3Im99s52BWHj1bNeCFmy5mUPvALrJe0VSK\nAKCq9qGfp4p0DciYjGN5zFi2g7eW7+RITgGXXdiI8YPa0jumoY0B56HCB4AaNWqQnp5OZGSk/QM4\nR6pKeno6NWpUjNWLTOjadziHaV9v59/f/0h2vochnS5g3OVt6RJdP9BNq9AqfACIjo4mJSWFtLS0\nQDelQqpRowbR0dGBboYxxdqVfozXvtrOBytT8KgytFszHhrYlnZN6gS6aZVChQ8A4eHhxMTEBLoZ\nxhg/2rzvCFOWJPPftc4i67f1ctI1tGgY3IusVzQVPgAYYyqP1T8eYnJ8Mgt/2E+tamGMHtCG+y+N\noXFdm6YsCxYAjDEBpap8m5zO5CVJLEtKp17NcH55ZTtGXtKa+hHVAt28Ss0CgDEmIAoLlUWbDzA5\nPok1uzNpVKc6v7+uA3f0aUntCrrIekVjf2VjTLkq8BTy2fq9TFmSzOZ9R4luUJMXbrqYW3pGUyM8\ndNM1BIIFAGNMucgt8PDRKiddw67047RrXJt//LwrN3RpVqnW2a1ILAAYY8rU8bwC3l2xm2lLt7Pv\nSA5douvx+oieXNWhcq6zW5FYADDGlInD2fnMWr6T6ct2cOh4Pn3bNOSvt3bh0tgoe2gzSFgAMMb4\nVdrRXKYv28Hsb3eRlVvA4IsaM/7ytvRsZUuUBhsLAMYYv0jNzGbqV8nMSdhNnqeQ6zs35aFBbenU\nrF6gm2bOwAKAMaZUktOyeG1JMh+tTkUEhnWP5sGBbWjTqHagm2ZK4FMAEJEhwMtAGPCGqr5UZHsr\nnIXgGwEZwN2qmuJuawm8AbQAFLhOVXe6i8zPASKBlcAIVc3zS6+MMWVu457DvBqfzPwNe6letQp3\n923FmMva0Kx+aK+zW5GUGABEJAyYDFwFpAAJIjJPVTd57TYJmKWqb4nIYGAiMMLdNgt4UVW/FJHa\nQKFb/mfgH6o6R0ReA+4HpvilV8aYMpO4M4PJ8UnEb0mjTvWqjBvUllH9Y4iqXT3QTTPnyJczgN5A\nkqpuBxCROcBQwDsAdAR+7b6OBz529+0IVHUXhkdVs9xyAQYDd7rHvAX8EQsAxgQlVWXptoNMjk9i\nxY4MGtaqxuPXOIus16tpi6xXVL4EgObAbq/fU4A+RfZZCwzDmSa6GagjIpHAhUCmiHwIxAALgQlA\nAyBTVQu86mxe3JuLyBhgDEDLli19aK4xxl8KC5UvNu1jcnwy61MP07ReDf5wQ0du79UypBdZryz8\ndRH4MeBfIjISWAqkAh63/gFAd+BH4D1gJPCJrxWr6lRgKkBcXJwtX2VMOcj3FDJvzR6mfJVM0oEs\nWkdG8Ofhnbm5e7Qtsl6J+BIAUnEu4J4Q7ZadpKp7cM4AcOf5h6tqpoikAGu8po8+BvriXDCuLyJV\n3bOA0+o0xpS/nHwP/1mZwutfJZNyKJuLLqjDP+/oznWdm9oi65WQLwEgAWjn3rWTCtzOT3P3AIhI\nFJChqoXAkzgD/Ilj64tII1VNw5n3T1RVFZF44BacO4Hu5RzOCowx/lFYqGzae4TlyQdZlpROws4M\njud56N6yPs/e2InBFzW2p3YrsRIDgKoWiMjDwAKc20Cnq+pGEXkOZzCfBwwCJoqI4kwBjXeP9YjI\nY8Ai98LvSmCaW/UTwBwReQFYDbzp364ZY4pSVXamH2dZ0kGWJx/k2+R0Dh3PB6Bto1rc0jOaay9u\nSt82tsh6KBDVijOtHhcXp4mJiYFuhjEVyoEjOSxPTncH/XRSM7MBaFqvBpe0jaJ/bCSXtI3ignq2\n6lZlJSIrVTWuaLk9CWxMJXMkJ5/vktNPDvrbDmQBUD8inH5tIhk7qC3920YSE1XLvuWHOAsAxlRw\nOfkeVu46xLKkgyxLTmd9SiaFCjXCq9A7JpJbekbTPzaKjk3rWvplcwoLAMZUMAWeQtanHj75DT9x\n1yHyCgoJqyJ0a1Gfhy+P5ZLYKLq3rE/1qnavvjkzCwDGBDlVJelAFsuSDvJNUjrf70jnaI7zDOVF\nF9RhRN9W9I+NpHdMpK2la86J/WsxJgilZmY7F23dC7cHjuYC0LJhBD/r0pRL2kbRr22k5d8xpWIB\nwJggkHEsj2+T01mW7Az6O9OPAxBVu9opd+q0aBgR4JaaysQCgDEBcCy3gBU7M1ie5DyAtWnvEQBq\nV69Kn5iGjOjXmv6xkbRvUsfu1DFlxgKAMeUgr6CQtSmZfLPNeQBrze5M8j1KtbAq9GhVn99cdSGX\nxEbRNboeVcMs144pHxYAjCkDhYXKD/uOsDzJmdZZscNJsSACnZvX4/5L29A/NpK4Vg0tq6YJGAsA\nxviBqrIr/bg7h5/Ot9vTyTjmLHB3IsXCJW2j6NcmknoRlj/fBAcLAMacpwNHc5xv+MWkWLi8fWNL\nsWCCngUAY3x0phQL9Wq6KRYGtqF/bJSlWDAVhgUAY84gJ9/Dql2H+KaYFAu9WjdkeM9o+reNomOz\nupYr31RIFgCMcXkKlfWph0+mSk7ceYhcS7FgKjELACZkeadYWJacznfbT02xcLelWDCVnP2rNiHl\nTCkWWjSsyfWdm9I/1lIsmNBhAcBUamdLsdCvbRT920bSP9ZSLJjQ5FMAEJEhwMs4S0K+oaovFdne\nCmcd4EZABnC3qqa42zzAenfXH1X1Rrd8JjAQOOxuG6mqa0rVGxPyjucVsGJHxsk7dTbtPYKqpVgw\npjglBgARCQMmA1cBKUCCiMxT1U1eu00CZqnqWyIyGJgIjHC3ZatqtzNU/7iqzj3/5ptQl+8pZM3u\nTHdaJ53Vuw+dkmLh11c6KRa6RNcj3FIsGHMKX84AegNJqrodQETmAEMB7wDQEfi1+zoe+NifjTTm\nhLOlWLi4maVYMOZc+BIAmgO7vX5PAfoU2WctMAxnmuhmoI6IRKpqOlBDRBKBAuAlVfUODi+KyDPA\nImCCquaeZz9MJXW2FAttGtVieI9o+sdG0rdNJPUjqgW4tcZULP66CPwY8C8RGQksBVIBj7utlaqm\nikgbYLGIrFfVZOBJYB9QDZgKPAE8V7RiERkDjAFo2bKln5prgtmBoznOhVs3VfKJFAsX1K3BoPaN\n6N82iv6xlmLBmNLyJQCkAi28fo92y05S1T04ZwCISG1guKpmuttS3f9uF5ElQHcgWVX3uofnisgM\nnCByGlWdihMgiIuLU9+6ZSqSIzn5fL894+QDWFv3n55i4ZLYKNpYigVj/MqXAJAAtBORGJyB/3bg\nTu8dRCQKyFDVQpxv9tPd8gbAcVXNdffpD/zF3dZUVfeK83/0TcAGP/XJVADH8wp4/avtfLU1jXVF\nUiwM62EpFowpDyUGAFUtEJGHgQU4t4FOV9WNIvIckKiq84BBwEQRUZwpoPHu4R2A10WkEKiCcw3g\nxMXjd0SkESDAGmCsH/tlgpinUHnk36tZvOUA3VvUZ/zlsfS3FAvGlDtRrTizKnFxcZqYmBjoZphS\nUFX+MG8js77dxfNDOzGiX+tAN8mYSk9EVqpqXNFyuzHalKs3v9nBrG93MXpAjA3+xgSYBQBTbv63\nYS8vzv+Bay++gCev7RDo5hgT8iwAmHKx+sdDPDpnDd1a1OcfP+9GFbu4a0zAWQAwZe7H9OM88FYi\nTerWYNo9cdQItwu9xgQDCwCmTGUez2PkzBUUFCozRvWyNMvGBBELAKbM5BZ4GDN7JSkZ2Uwd0ZO2\njWoHuknGGC+2HoApE6rKb+euY8WODF6+vRt92kQGuknGmCLsDMCUib9/uZVP1uzh8WvaM7Rb80A3\nxxhTDAsAxu/eT9jNPxcncXuvFowb1DbQzTHGnIEFAONXX29L43cfrWdAuyiev+liS95mTBCzAGD8\nZvO+I4x7exWxjWvz6l09bAUuY4Kc/R9q/GL/kRzum5FARPUwpo/sRZ0a4YFukjGmBBYATKkdyy3g\nvpkJHM7OZ/rIXjSrXzPQTTLG+MBuAzWlUuAp5JF3V7N531HeuDeOTs3qBbpJxhgf2RmAOW+qyh//\nu5HFmw/w3NBOXN6+caCbZIw5BxYAzHl74+sdvP3djzw4sA139WkV6OYYY86RBQBzXuavd1I7X9+5\nKU9cc1Ggm2OMOQ8WAMw5W7krg1++t4aerRrwt9u6WmpnYyoonwKAiAwRkS0ikiQiE4rZ3kpEFonI\nOhFZIiLRXts8IrLG/ZnnVR4jIt+7db4nItX80yVTlnYePMboWStpVs9SOxtT0ZUYAEQkDJgMXAt0\nBO4QkY5FdpsEzFLVLsBzwESvbdmq2s39udGr/M/AP1Q1FjgE3F+KfphycOhYHqNmJqCqzBjVm4a1\nLGYbU5H5cgbQG0hS1e2qmgfMAYYW2acjsNh9HV/M9lOIkx9gMDDXLXoLuMnXRpvyl5PvYczsRFIz\ns5l2TxwxUbUC3SRjQsex9DKp1pcA0BzY7fV7ilvmbS0wzH19M1BHRE7k/60hIoki8p2InBjkI4FM\nVS04S50AiMgY9/jEtLQ0H5pr/K2wUHl87joSdh7ib7d2Ja51w0A3yZjKrdADuxNg8Yvw+kD4axs4\ntMvvb+OvB8EeA/4lIiOBpUAq4HG3tVLVVBFpAywWkfXAYV8rVtWpwFSAuLg49VN7zTmY9MUW/rt2\nD08MuYgbujYLdHOMqZyOZ0DSItj2BSQthOwMkCoQ3RsGPw3h/n/C3pcAkAq08Po92i07SVX34J4B\niEhtYLiqZrrbUt3/bheRJUB34AOgvohUdc8CTqvTBId3V/zIq0uSuaN3S8YObBPo5hhTeRQWwr51\nsO1LZ9BPTQQthIhIaHc1tLsK2g6GiLI74/YlACQA7UQkBmeQvh2403sHEYkCMlS1EHgSmO6WNwCO\nq2quu09/4C+qqiISD9yCc03hXuATP/XJ+MlXW9N46uMNDLywEc8P7WSpnY0prZzDkBzvDPpJX0LW\nfqe8WQ+47LfOwN+sG1Qpn7vrSgwAqlogIg8DC4AwYLqqbhSR54BEVZ0HDAImiojiTAGNdw/vALwu\nIoU41xteUtVN7rYngDki8gKwGnjTj/0ypbRpzxHGv7OKC5vUYfJdPahqqZ2NOXeqcOAH5xv+ti9h\n93dQWAA16kHbK5wBP/YKqB2YNCqiWnGm1ePi4jQxMTHQzaj09h3O4abJywD4aPwlNK1n2T2N8Vne\nMdixFLYucAb9IylOeZPOzrROu6shuheElV8uThFZqapxRcstG6g5RVZuAaNmJpCVW8D7D/azwd8Y\nX6Qnu9/yv4Cd34AnD6rVhjaDYOBvIfZKqBd8a2NbADAnFXgKGf/OKrbuP8r0kb3o2KxuoJtkTHDK\nz4Fd3/x0ATdju1Me1R56j3G+5bfsB1WD+2FJCwAGcFI7P/3JRr7amsbEYZ0ZeGGjQDfJmOByaJdz\n4Xbbl7D9KyjIhqo1IeYy6DvOmd5p0DrQrTwnFgAMAK8v3c67K35k3KC23NG7ZaCbY0zgFeQ5F21P\nXMBN2+yUN2gNPe5xvuW37l8m9+eXFwsAhk/X7eGlzzdzQ9dmPHZ1+0A3x5jAObLX/Zb/BSQvgbyj\nEFYNWvWHHvc63/IjY6GS3BJtASDEJe7M4Nfvr6VX6wb89ZYultrZhBZPgfMA1okLuPvWO+V1m0Pn\n4dDuGmeKp3rtwLazjFgACGE7Dh5j9KxEmtevydQRltrZhIisNEg+kXJhEeRkgoRBy75w5R+dqZ3G\nHSvNt/yzsQAQojKO5TFqxgpEhBkje9HAUjubyqqwEPau9kq5sApQqNUYLrremdZpcznUrB/olpY7\nCwAhKCffw+hZiew5nMO7o/vS2lI7m8om+xAkL3YH/S/h+EFAIDoOLv+dM+hf0BWqhPYT7hYAQkxh\nofKb/6xl5a5DvHpXD3q2ahDoJhlTeqqwf4NXyoXvncRqNRs4D2G1u9pJvVArsuS6QogFgBDzlwVb\n+GzdXn533UVc17lpoJtjzPnLPQrbl/w06B/d65Q37QoDfuMM+s17lltitYrIAkAIeef7Xbz2VTJ3\n923J6AGW2tlUMKpwcOtPd+zs+hYK86F6XWh7uZtY7Uqoc0GgW1phWAAIEfFbDvDMJxu5vH0j/niD\npXY2FUTecdj59U+DfuaPTnnjjtBvnDPot+gDYeGBbWcFZQEgBGxIPcz4d1Zx0QV1+NedltrZBLmM\n7T/dsbPja/DkQniEk1jt0vaVBPkAABjmSURBVF9B7FVQv0VJtRgfWACo5PZkZnP/WwnUqxnO9JG9\nqFXdPnITZApyYdeynwb99CSnPDIWet3v3LHTqj9UrR7YdlZCNhpUYkdz8rlvZgLHcj3MfagfTerW\nCHSTjHFk7j41sVr+MQirDjEDnGyasVdCZNtAt7LSswBQSeV7Chn3ziqSDmQxY1QvLrrAUjubAPLk\nO7dmnrhj54C7MGC9ltDtDjex2gCoFhHYdoYYCwCVkKry9Mcb+HrbQf48vDMD2llqZxMAR/dB0kI3\nsVo85B6BKlWh1SVw9QvOoB91YUikXAhWPgUAERkCvIyzJvAbqvpSke2tcBaCbwRkAHeraorX9rrA\nJuBjVX3YLVsCNAWy3d2uVtUDpeqNAeDVJcnMSdjNw5fH8vNeltrZlJNCD6Su/OmOnb1rnfI6TaHT\nTc6AHzMQatjZaLAoMQCISBgwGbgKSAESRGSe1+LuAJOAWar6logMBiYCI7y2P4+zWHxRd6mqLfLr\nR5+sSeWvC7YwtFszfnP1hYFuTvFUIeENWPKS87RmeIRz6h8eAdVqOfnVT76OcH4/8brYMvfYotvt\nm2XZO5bulVhtoZOCQao4t2Ze8Ywz6De52D6LIOXLGUBvIElVtwOIyBxgKM43+hM6Ar92X8cDH5/Y\nICI9gSbA/4DTFiU2/rNiRwaP/2cdvWMa8pdbugTnvf7HM2DeI7D5U2fOt1F7517v/GPuf7PheDrk\n7Yb8485P3nFn9aVzdTIoREB4rbMEDe/t7uvitnuXVa0emoNaYSHsW/vTHTspiYBCRBRcOOSnxGoR\nDQPdUuMDXwJAc2C31+8pQJ8i+6wFhuFME90M1BGRSOAQ8DfgbuDKYuqeISIe4APgBVXVojuIyBhg\nDEDLljadcSbJaVmMmZ1IdMOaTB3Rk+pVg/Dx913L4YPRkLUfrn7RWUbP12RchYVuQMj2ChZeAcI7\ngHhvzzt2elnWvtO3e3LPrS9S5QxBo+apweLkGU1xQeUs24NpLdnsTNge/1NitWMHAIHmPWDQBGfQ\nb9o95BOrVUT+ugj8GPAvERmJM9WTCniAccB8VU0p5tvoXaqaKiJ1cALACGBW0Z1UdSowFSAuLu60\nAGEgPSuXUTMSCBNh5sje1I8IosEDnLnhpZPgq5ec5fTu/8IZPM5FlSrOohzVa+NcavIzT8FPAeVk\nUPEOICdeF7fdDTwnXh8/dHpwKiw4x/5W9SFonG2arMj2otNkZ8uPo+rcpXPijp0fvwP1QI36EHvF\nT4nVatvNBRWdLwEgFfB+7C7aLTtJVffgnAEgIrWB4aqaKSL9gAEiMg6oDVQTkSxVnaCqqe6xR0Xk\n3zhTTacFAHN2OfkeHpiVyP4jOcwZ05eWkUF2G93hVPhwDOz6Brr8HK7/G1SvE+hWnS6sKoTVLbsL\nlJ78nwLEKWcvxQSQE0HjtLLjkJcFWQe8trv7auE59rf6GabEakLaFjji/i9+QWe49JduYrU45+9k\nKg1fPs0EoJ2IxOAM/LcDd3rvICJRQIaqFgJP4twRhKre5bXPSCBOVSeISFWgvqoeFJFw4GfAQj/0\nJ6QUFiq/em8Na3ZnMuWuHnRvGWSpnTfPh0/GOYtr3/Sac793qAoLdxYcKYtFR1Sdp2mLnnV4Bwhf\np8yyM3+a2om9Euo28397TdAoMQCoaoGIPAwswLkNdLqqbhSR54BEVZ0HDAImiojiTAGNL6Ha6sAC\nd/APwxn8p51/N0LTS//bzOcb9vHU9R0YcnEQpXbOz4Evn4EVr8MFXeCWGRAVG+hWVV4iEF7D+cEu\nvhrfSTHXXYNWXFycJibaXaMAs7/dydOfbOTefq34441BlN3z4Db4zyjYv965yHvlHy2HizEBJiIr\nVfW0uzBtQq8CWrx5P3+Yt5ErOzTmmWBJ7awKa96B+Y8788h3vAfthwS6VcaYs7AAUMFsSD3Mw/9e\nTadm9Xjlju6EVQmCwT/nCHz6K9gw17m3f9g0qBtEU1LGmGJZAKhAUjOzuW9mAg0iqvHmvXFEVAuC\njy91Jcy9z8nuOPgpuPTXtgSfMRVEEIwgxhdHcvK5b0YC2Xke3h7Xh8aBTu1cWAjf/hMWPefkehk1\nH1r2DWybjDHnxAJABZDvKWTc26tITsvirft6c2GTAN9Hn3UAPhrr5IDpcCPc+ArUDLJbUI0xJbIA\nEORUld9/tJ5vkg7y11u60D82KrANSl4MHz7opPa9/u8Qd19o5sQxphKwABDkJscn8X5iCr+4oh23\nxgVwHVRPPix+Hpa9DI0ugns+gSYdA9ceY0ypWQAIYh+vTmXSF1sZ1r05v7qyXeAakrEDPrjfueDb\ncyRcM9FWbjKmErAAEKS+257O43PX0rdNQ14aHsDUzhs+gP/+EhC4dSZ0ujkw7TDG+J0FgCCUdOAo\nY2Yl0rJhBK/fHUe1qgFIs5t3DD5/AlbPhujeMPwNaNCq/NthjCkzFgCCTNrRXEbOSKBa1SrMHNWb\nehHh5d+IfRtg7ignrcOA38CgJ51kZsaYSsUCQBDJznNSOx/MyuW9Mf1o0bCc59lPLNW44PfObZ33\nfAxtBpVvG4wx5cYCQJDwFCq/fG8161Iyef3unnRtUQZpg8/meAZ88jBs+czJ/X7TFKgV4FtOjTFl\nygJAkPjT/B9YsHE/f7ihI1d3uqB833zXcvjgAecBr2v+BH0esuX9jAkBFgCCwMxlO3jzmx2M6t+a\nUf1jyu+NCz2w9K/w1Z+dpRof+BKadS+/9zfGBJQFgAD7ctN+nvt0E1d1bMJT15fjg1WHU+HD0bBr\nGXS5Ha6fFJxLNRpjyowFgABal5LJL95dTefm9Xj59m7ll9rZe6nGm1+HrreXz/saY4KKTxO9IjJE\nRLaISJKITChmeysRWSQi60RkiYhEF9leV0RSRORfXmU9RWS9W+crEhSrmpSflEPHuW9mIpG1q/HG\nvb3KJ7Vzfo6zYMucO6B+Sxj7tQ3+xoSwEgOAiIQBk4FrgY7AHSJSdK5iEjBLVbsAzwETi2x/Hmet\nYG9TgNFAO/cnZJaPOpydz6gZCeQVeJg5qheN6pTDkolpW+GNK2HFVGepxvu/hMi2Zf++xpig5csZ\nQG8gSVW3q2oeMAcYWmSfjsBi93W893YR6Qk0Ab7wKmsK1FXV79RZlHgWcNN596ICySso5KG3V7Iz\n/RivjehJbOMynndXhdVvw9SBcHQP3Pk+DJlo6/QaY3wKAM2B3V6/p7hl3tYCw9zXNwN1RCRSRKoA\nfwMeK6bOlBLqrHRUlSc/XM/y5HT+PLwLl7Qt4/vscw47t3d+Mh6a94Sxy+DCa8r2PY0xFYa/bvZ+\nDBgoIquBgUAq4AHGAfNVNeVsB5+NiIwRkUQRSUxLS/NPawPklUVJfLAqhV9deSHDekSXfEBppKyE\n1wbAxo+cpRrv+cTW6TXGnMKXK4+pgHci+mi37CRV3YN7BiAitYHhqpopIv2AASIyDqgNVBORLOBl\nt54z1ulV91RgKkBcXJz60qlg9MHKFP6xcCvDe0Tziytiy+6NCgth+StO7v46TWHU59CyT9m9nzGm\nwvIlACQA7UQkBmeQvh2403sHEYkCMlS1EHgSmA6gqnd57TMSiFPVCe7vR0SkL/A9cA/wz1L3Jkgt\nTz7IhA/XcUnbSCYO61x2qZ2zDsBHDzqrdtlSjcaYEpQ4BaSqBcDDwALgB+B9Vd0oIs+JyI3uboOA\nLSKyFeeC74s+vPc44A0gCUgGPj/35ge/bfuP8uDslbSOrMWUu3uWXWrnpEUwpb+T1uFn/4DbZtng\nb4w5K3FuwqkY4uLiNDExMdDN8NmBozncPHk5eZ5CPhp3CdENyiC7Z0EexL/gLtXYAW6Zbks1GmNO\nISIrVTWuaLk9CVxGjucV8MBbiWQcy+O9B/uWzeB/ylKNo5xEbrZUozHGRxYAyoCnUHl0zho2pB5m\n6og4ukSXQWrn9XPh01+BCNz6FnQKiccojDF+ZAGgDLzw2Sa+3LSfZ2/sxJUdm/i38rxj8PlvnYe7\nWvRxlmqs39K/72GMCQkWAPxs+jc7mLFsJ/dfGsO9l7T2b+X71sPc+9ylGh9zl2q0j9AYc35s9PCj\nBRv38fxnm7imUxN+d10H/1WsCiumwRdPuUs1fgJtBvqvfmNMSLIA4Cdrdmfy6JzVdI2uz//9vLv/\nUjvbUo3GmDJiAcAPdmcc54G3EmhUpzpv3BtHzWph/ql45zJn0ZasA3DNROj7kHPR1xhj/MACQCkd\nPp7PyBkryPcoc0b2Jqq2H7JsegqcpRqX/gUaxMADC6FZt9LXa4wxXiwAlEJugYcH305kd0Y2s+/v\nTWzj2qWv9HAKfDAaflxuSzUaY8qUBYDzpKpM+GA9323P4OXbu9GnTWTpK938mZO62ZNvSzUaY8qc\nBYDz9I+F2/hodSqPXX0hQ7uVcimD/Bz48mlnta6mXeGWGbZalzGmzFkAOA//SdzNK4u2cVtcNOMv\nL2Vq57Qtzr39+zdA3/Fw5R9stS5jTLmwAHCOliUd5MkP13NpbBQv3lyK1M4nlmr8/LcQXhPu/A9c\neLV/G2uMMWdhAeAcbNl3lLGzV9K2UW1evbsH4WHnmdo557CTx2fDBxBzGdw81VbrMsaUOwsAPjpw\nJIf7ZiZQs1oY00f1om6N8POrKGUlzB3l3O0z+Gm49FdQxU/PDRhjzDmwAOCDY7kF3PdWAoeO5/H+\ng/1oXr/muVdyylKNzWypRmNMwFkAKIGnUPnFu6vZtOcIb9wbx8XN6517JUf3O0s1bo+HjkPhhleg\nZhmkiDbGmHNgAeAsVJVn/7uRRZsP8PzQTgy+6DxSOyctcgb/3KPws/+DniMtnYMxJij4dBVTRIaI\nyBYRSRKRCcVsbyUii0RknYgsEZFor/JVIrJGRDaKyFivY5a4da5xfxr7r1v+8eY3O5j17S7GXNaG\nEf1an9vBBXnwxdPw9jCIiIIxSyBulA3+xpigUeIZgIiEAZOBq4AUIEFE5qnqJq/dJgGzVPUtERkM\nTARGAHuBfqqaKyK1gQ3usXvc4+5S1aBc5Pd/G/by4vwfuPbiC5gw5KJzOzhjh3Nv/55VEHefs1Rj\n+HlcNzDGmDLkyxRQbyBJVbcDiMgcYCjgHQA6Ar92X8cDHwOoap7XPtXx8Ywj0Fb/eIhH56yhW4v6\n/OPn3ahyLqmd18+F//4SqlSB22Y5c/7GGBOEfBmQmwO7vX5Pccu8rQWGua9vBuqISCSAiLQQkXVu\nHX/2+vYPMMOd/nlazvBElYiMEZFEEUlMS0vzobml82P6cR54K5EmdWvwxj1x1Aj38RbNvGPw8Xhn\nkfYmHWHsNzb4G2OCmr++kT8GDBSR1cBAIBXwAKjqblXtAsQC94rIiSupd6lqZ2CA+zOiuIpVdaqq\nxqlqXKNGjfzU3OJlHs9j5MwVeFSZOaoXkb6mdt67Dl4fCGvegcseh5HzbZ1eY0zQ8yUApAItvH6P\ndstOUtU9qjpMVbsDv3fLMovuA2zAGexR1VT3v0eBf+NMNQVMboGHMbNXkpKRzdQRcbRp5ENqZ1X4\nfiq8cQXkZcG982DwU7ZOrzGmQvAlACQA7UQkRkSqAbcD87x3EJEoETlR15PAdLc8WkRquq8bAJcC\nW0SkqohEueXhwM9wgkNAqCq/nbuOFTsymHRbV3rHNCz5oOMZMOdO+PxxaHO5M+UTc1nZN9YYY/yk\nxK+qqlogIg8DC4AwYLqqbhSR54BEVZ0HDAImiogCS4Hx7uEdgL+55QJMUtX1IlILWOAO/mHAQmCa\nn/vms79/uZVP1uzht0Pac2PXZiUfsPMbZ9GW4wdhyEvQZ6zd3mmMqXBEVQPdBp/FxcVpYqJ/7xp9\nP2E3v/1gHXf0bsGfSsruWXSpxlum21KNxpigJyIrVTWuaHlIT1Z/vS2N3320nssubMRzQy8+++Dv\nvVRj1zvhur/YUo3GmAotZAPA5n1HeOjtVcQ2rs3kO7ufPbXz5s/g43FQWOCkbu768/JrqDHGlJGQ\nDAD7j+QwakYCtaqHMWNUL+qcKbVzfg588RQkTIOm3ZwpH1uq0RhTSYRcAMjKLWDUjASOZOfz/th+\nNK13hhQN3ks19nsYrvgDVK1Wvo01xpgyFFIBoMBTyCP/XsWW/Ud54944OjUrJrWzKqyeDZ8/AeER\ncNdcaHdV+TfWGGPKWMgEAFXlD/M2Er8ljT/d3JnL2xeTfDTnsJPHZ+OHEDMQhk2FOheUf2ONMaYc\nhEwAmPb1dt75/kfGDmzLnX2KSdOQkuhM+RxOgSuegf6/tKUajTGVWkgEgM/W7eVP8zdzfZem/Paa\n9qduLCyE5S/D4hecpRrv+x+0CGhWCmOMKReVPgCoKh+uSqFnqwb87daup6Z2PmWpxpvghpdtqUZj\nTMio9AFARJhyd0+y8zynpnZOWggfjYXcLGfg73GvpXMwxoSUSh8AAKpVrUK1qu6DXgV5sPg5WP5P\naNwR7v0UGp/jil/GGFMJhEQAOCljO8y9312q8X645kVbqtEYE7JCJwCcslTjbOh4Y6BbZIwxAVX5\nA4AqfPpLWDkTWvSF4dNstS5jjCEUAoAINGzrLNU4cIKt1mWMMa7QGA37/yLQLTDGmKDjr0XhjTHG\nVDAWAIwxJkT5FABEZIiIbBGRJBGZUMz2ViKySETWicgSEYn2Kl8lImtEZKOIjPU6pqeIrHfrfEXO\nuhyXMcYYfysxAIhIGDAZuBboCNwhIh2L7DYJmKWqXYDngIlu+V6gn6p2A/oAE0TkxKrrU4DRQDv3\nZ0gp+2KMMeYc+HIG0BtIUtXtqpoHzAGGFtmnI7DYfR1/Yruq5qlqrlte/cT7iUhToK6qfqfOqvSz\ngJtK1RNjjDHnxJcA0BzY7fV7ilvmbS0wzH19M1BHRCIBRKSFiKxz6/izqu5xj08poU7c48eISKKI\nJKalpfnQXGOMMb7w10Xgx4CBIrIaGAikAh4AVd3tTg3FAveKSJNzqVhVp6pqnKrGNWrUyE/NNcYY\n48tzAKlAC6/fo92yk9xv9cMARKQ2MFxVM4vuIyIbgAHAMreeM9ZpjDGmbPkSABKAdiISgzNI3w7c\n6b2DiEQBGapaCDwJTHfLo4F0Vc0WkQbApcA/VHWviBwRkb7A98A9wD9LasjKlSsPisgu37t3iijg\n4HkeG2wqS18qSz/A+hKsKktfStuPVsUVlhgAVLVARB4GFgBhwHRV3SgizwGJqjoPGARMFBEFlgLj\n3cM7AH9zywWYpKrr3W3jgJlATeBz96ektpz3HJCIJKpq3PkeH0wqS18qSz/A+hKsKktfyqofPqWC\nUNX5wPwiZc94vZ4LzC3muC+BLmeoMxG4+Fwaa4wxxn/sSWBjjAlRoRQApga6AX5UWfpSWfoB1pdg\nVVn6Uib9EOc5LGOMMaEmlM4AjDHGeKl0AcCHxHXVReQ9d/v3ItK6/FtZMh/6MVJE0txEe2tE5IFA\ntNMXIjJdRA64z4EUt13chIBJbkLBHuXdRl/40I9BInLY6zN5prj9goH7hH68iGxyEzU+Wsw+Qf+5\n+NiPCvG5iEgNEVkhImvdvjxbzD7+Hb9UtdL84Nymmgy0AarhpKjoWGSfccBr7uvbgfcC3e7z7MdI\n4F+BbquP/bkM6AFsOMP263BuAxagL/B9oNt8nv0YBHwa6Hb62JemQA/3dR1gazH/xoL+c/GxHxXi\nc3H/zrXd1+E4z0j1LbKPX8evynYG4EviuqHAW+7rucAVQZiK2pd+VBiquhTIOMsuQ3GyyaqqfgfU\ndxMGBhUf+lFhqOpeVV3lvj4K/MDp+biC/nPxsR8Vgvt3znJ/DXd/il6k9ev4VdkCgC+J607uo6oF\nwGEgslxa5ztf+gEw3D01nysiLYrZXlH42t+KoJ97Cv+5iHQKdGN84U4jdMf5xumtQn0uZ+kHVJDP\nRUTCRGQNcAD4UlXP+Jn4Y/yqbAEglPwXaK1Oor0v+elbgQmcVUArVe2Kk9rk4wC3p0Ru7q4PgF+q\n6pFAt+d8ldCPCvO5qKpHnfVTooHeIlKmD8tWtgBQYuI6731EpCpQD0gvl9b5zpcEfOn601oLbwA9\ny6ltZcGXzy3oqeqRE6fw6jw9H+7myQpKIhKOM2i+o6ofFrNLhfhcSupHRftcANRJphnP6Qtl+XX8\nqmwB4GTiOhGphnORZF6RfeYB97qvbwEWq3tFJYiU2I8ic7E34sx9VlTzgHvcu076AodVdW+gG3Wu\nROSCE/OxItIb5/+vYPtyATh3+ABvAj+o6t/PsFvQfy6+9KOifC4i0khE6ruvawJXAZuL7ObX8cun\nXEAVhfqWuO5NYLaIJOFc0Ls9cC0uno/9+IWI3AgU4PRjZMAaXAIReRfnTowoEUkB/oBzgQtVfQ0n\nz9R1QBJwHBgVmJaenQ/9uAV4SEQKgGzg9iD8cnFCf2AEsN6dcwb4HdASKtTn4ks/Ksrn0hR4S5xl\neKsA76vqp2U5ftmTwMYYE6Iq2xSQMcYYH1kAMMaYEGUBwBhjQpQFAGOMCVEWAIwxJkRZADDGmBBl\nAcAYY0KUBQBjjAlR/w+VCjBQCmx67QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iCWPdGNiitw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plFU40N4ijY4",
        "colab_type": "text"
      },
      "source": [
        "Note the difference between training and test accuracy. Some form of regularization should be used to improve the generalization of the model. \n",
        "Examples of regularization to explore include dropout, l1-norm, and l2-norm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzRH9FbEpiX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdT0Wyw9SPak",
        "colab_type": "text"
      },
      "source": [
        "## Keras Example\n",
        "In this case I also demonstrate an example using the Keras library and in this one I demonstrate the method of running the samples through the reservoir and then classifying using a separate model as a readout layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCAP-0ewpiYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "5f42cb5b-aa4c-428d-e93d-36bedc43c2d5"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, SimpleRNN, Flatten\n",
        "from keras import initializers\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRbzzE1QpiYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1bd147b3-ea8c-4106-9ced-7964108f360e"
      },
      "source": [
        "num_classes = 10\n",
        "img_rows, img_cols = 28, 28\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# some data preprocessing and normalizing\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLQhrue-piYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "from keras import regularizers\n",
        "  \n",
        "# for reproducibility\n",
        "set_seed = 0\n",
        "seed(set_seed)\n",
        "set_random_seed(set_seed)\n",
        "\n",
        "n_hidden = 50\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "bias_initializer = 'zeros'\n",
        "dropout = 0.0\n",
        "# this returns all of the reservoir output values when set to True\n",
        "return_sequences = True\n",
        "\n",
        "l1 = 0\n",
        "l2 = 0\n",
        "\n",
        "INPUT_SIZE = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "# network is initialized with specific weights and is then only trained\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(n_hidden, activation='tanh', \n",
        "                    kernel_initializer=initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=set_seed),\n",
        "                    bias_initializer=bias_initializer,\n",
        "                    input_shape=INPUT_SIZE,\n",
        "                    return_sequences=return_sequences,\n",
        "                    unroll=False, stateful=False))\n",
        "\n",
        "# In order for the reservoir to have Echo State property the \n",
        "# spectral should be <1\n",
        "spectral_radius = 0.9\n",
        "sparsity = 1.0\n",
        "\n",
        "# gets the weights from the model and generates an effective recurrent weight layer\n",
        "rnn_weights = model.get_weights()\n",
        "esn_recurrent_weights = create_reservoir_matrix(size=rnn_weights[1].shape,\n",
        "                                                  spectral_radius=spectral_radius,\n",
        "                                                  sparsity=sparsity)\n",
        "# assigns the new weights for the recurrent layer to the model and keeps input\n",
        "# weights the same\n",
        "rnn_weights[1] = esn_recurrent_weights\n",
        "model.set_weights(rnn_weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhEEUhEwtvby",
        "colab_type": "text"
      },
      "source": [
        "Now after building the model and replacing the weights, we can run the inputs through the network and save the output values from each of the reservoir nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYrO3P9Uydm",
        "colab_type": "code",
        "outputId": "b49596d5-9cbe-40aa-e493-bcdb66a41520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "# propagate through all the inputs and return final output of cells\n",
        "reservoir_states_train = model.predict(x_train, batch_size=int(len(x_train)/16), verbose=1)\n",
        "reservoir_states_test = model.predict(x_test, batch_size=int(len(x_test)/16), verbose=1)\n",
        "  \n",
        "print(final_state_train.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 1s 12us/step\n",
            "10000/10000 [==============================] - 0s 30us/step\n",
            "(60000, 28, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4RXk2YV2Tq",
        "colab_type": "text"
      },
      "source": [
        "### Final readout layer\n",
        "Here we can use another network to be the readout layer. There are other libraries which can be useful here as well such as with "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3I4PZMVUyv7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "outputId": "989cf3d7-c4a1-4a69-d81e-1306c864287f"
      },
      "source": [
        "# create the final readout layer, this uses logistic regression\n",
        "\n",
        "model_final_layer=Sequential()\n",
        "# converts the sequence of reservoir outputs to a 1D matrix\n",
        "if return_sequences:\n",
        "  model_final_layer.add(Flatten())\n",
        "model_final_layer.add(Dropout(dropout)) \n",
        "model_final_layer.add(Dense(num_classes, activation='softmax',\n",
        "                            activity_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
        "                      \n",
        "# set the learning rate and optimizer with some early stopping\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "earlystopper = EarlyStopping(monitor='val_acc', patience=10, verbose=1)\n",
        "\n",
        "model_final_layer.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=adam,\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "history = model_final_layer.fit(reservoir_states_train, y_train, epochs=epochs, \n",
        "                      batch_size=batch_size, \n",
        "                      validation_data=(reservoir_states_test, y_test), verbose=2, \n",
        "                      shuffle=True, callbacks=[earlystopper])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            " - 4s - loss: 0.4397 - acc: 0.8719 - val_loss: 0.2965 - val_acc: 0.9129\n",
            "Epoch 2/15\n",
            " - 3s - loss: 0.2943 - acc: 0.9143 - val_loss: 0.2564 - val_acc: 0.9238\n",
            "Epoch 3/15\n",
            " - 3s - loss: 0.2674 - acc: 0.9220 - val_loss: 0.2412 - val_acc: 0.9289\n",
            "Epoch 4/15\n",
            " - 3s - loss: 0.2520 - acc: 0.9270 - val_loss: 0.2328 - val_acc: 0.9298\n",
            "Epoch 5/15\n",
            " - 3s - loss: 0.2401 - acc: 0.9300 - val_loss: 0.2309 - val_acc: 0.9327\n",
            "Epoch 6/15\n",
            " - 3s - loss: 0.2321 - acc: 0.9323 - val_loss: 0.2152 - val_acc: 0.9350\n",
            "Epoch 7/15\n",
            " - 3s - loss: 0.2252 - acc: 0.9339 - val_loss: 0.2138 - val_acc: 0.9339\n",
            "Epoch 8/15\n",
            " - 3s - loss: 0.2202 - acc: 0.9359 - val_loss: 0.2061 - val_acc: 0.9355\n",
            "Epoch 9/15\n",
            " - 3s - loss: 0.2170 - acc: 0.9365 - val_loss: 0.2073 - val_acc: 0.9364\n",
            "Epoch 10/15\n",
            " - 3s - loss: 0.2138 - acc: 0.9372 - val_loss: 0.2026 - val_acc: 0.9387\n",
            "Epoch 11/15\n",
            " - 3s - loss: 0.2111 - acc: 0.9388 - val_loss: 0.1961 - val_acc: 0.9408\n",
            "Epoch 12/15\n",
            " - 3s - loss: 0.2064 - acc: 0.9385 - val_loss: 0.1974 - val_acc: 0.9391\n",
            "Epoch 13/15\n",
            " - 3s - loss: 0.2037 - acc: 0.9402 - val_loss: 0.1984 - val_acc: 0.9394\n",
            "Epoch 14/15\n",
            " - 3s - loss: 0.2017 - acc: 0.9413 - val_loss: 0.2022 - val_acc: 0.9380\n",
            "Epoch 15/15\n",
            " - 3s - loss: 0.2022 - acc: 0.9399 - val_loss: 0.1918 - val_acc: 0.9411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2cDYnXtUy9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "55fab301-749e-4229-d07b-5475d3b21647"
      },
      "source": [
        "# plot history\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['acc'], label='train accuracy')\n",
        "plt.plot(history.history['val_loss'], label='test loss')\n",
        "plt.plot(history.history['val_acc'], label='test accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}