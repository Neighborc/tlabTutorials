{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Reservoir_MNIST_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neighborc/tlabTutorials/blob/master/Reservoir_MNIST_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sp1UmWDGJvTl",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B9S1Sp7OclP",
        "colab_type": "text"
      },
      "source": [
        "## Reservoir\n",
        "Here I give a walkthrough of how to use PyTorch for constructing a reservoir, essentially a randomized recurrent neural network, for classification. A more in-depth implementation which could be good for reference is [EchoTorch](https://github.com/nschaetti/EchoTorch)\n",
        "\n",
        "The main steps I use are to initialize a RNN with standard nodes, replace the weight matrix only the readout layer is trained. \n",
        "\n",
        "There are many other parameters to tune."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AoAJfbfQaTgV",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.utils.data as utils\n",
        "\n",
        "\n",
        "# set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_HVHg0tlaWyg"
      },
      "source": [
        "First in order to use PyTorch effectively we want to it up to use the availabe GPU device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xaIk5KTyaXZ0",
        "outputId": "e52c2042-dba3-4350-bae0-08f341afaf6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Setup the ability to run on a GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xn3jGPqTa5Qo"
      },
      "source": [
        "## Constructing the reservoir \n",
        "Here we create a recurrent neural network, set its internal weights according to the Echo State Property and then freeze all the weights except the output. \n",
        "Additionally aother simple way of doing this is by removing the final layer and running the data through the reservoir and then training a classifier on these node output values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dTLyeuYcaX0X",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, num_layers, num_classes, sequence_length, dropout=0, return_sequence=False):\n",
        "        super(RNN, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_layers = num_layers\n",
        "        self.return_sequence = return_sequence\n",
        "          \n",
        "        self.hidden = nn.RNN(input_size, n_hidden, num_layers, batch_first=True, bias=False)\n",
        "        self.drop_layer = nn.Dropout(p=dropout)\n",
        "        if return_sequence:\n",
        "          self.fc = nn.Linear(n_hidden*sequence_length, num_classes)\n",
        "        else:\n",
        "          self.fc = nn.Linear(n_hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states \n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.n_hidden).to(device) \n",
        "        \n",
        "        # Forward propagate\n",
        "        out, h_n = self.hidden(x, h0)  # out: tensor of shape (batch_size, seq_length, n_hidden)\n",
        "        if self.return_sequence:\n",
        "          out = out.contiguous().view(-1,self.n_hidden*self.sequence_length)\n",
        "          out = self.fc(out)\n",
        "        else:\n",
        "          out = self.fc(out[:,-1,:])\n",
        "        return out\n",
        "\n",
        "def create_reservoir_matrix(size=(10,10), spectral_radius=0.9, sparsity=0.5):\n",
        "    \"\"\"\n",
        "    inputs: \n",
        "    size: square matrix representing the size of teh reservoir connections\n",
        "    spectral_radius: largest eigenvalue in reservoir matrix should be <1\n",
        "    sparsity: connectivity of matrix, 1.0 indicates full connection \n",
        "    \"\"\"\n",
        "    # generate uniformly random from U[0,1] *2 -1 makes it U[-1,1]\n",
        "    W_res = torch.rand(size)*2-1\n",
        "\n",
        "    # create a sparse mask array then multiply reservoir weights by sparse mask\n",
        "    # sparse mask is done by generating uniform[0,1] then setting True <=sparsity\n",
        "    W_res = W_res*(torch.rand_like(W_res) <= sparsity).type(W_res.dtype)\n",
        "    \n",
        "    # scale the matrix to have the desired spectral radius\n",
        "    W_res = W_res*spectral_radius/(np.max(np.abs(np.linalg.eigvals(W_res))))\n",
        "\n",
        "    return W_res\n",
        "\n",
        "\n",
        "def epoch_accuracy(loader, total_size, split='train'):\n",
        "  \"\"\"\n",
        "  evaluates the accuracy of the model on the dataset\n",
        "  \"\"\"\n",
        "    model.eval()\n",
        "    # Measure the accuracy for the entire dataset \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sequences, labels in loader:\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(sequences) \n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        total += labels.size(0) \n",
        "        correct += (predicted == torch.max(labels,1)[1]).sum()\n",
        "\n",
        "    print(f'Accuracy of the model on the {total_size} {split} sequences: {float(correct) / total:3.1%}')\n",
        "    \n",
        "    return float(correct) / total    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axtSmtH8piXg",
        "colab_type": "text"
      },
      "source": [
        "### MNIST Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnzbsEo3piXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8yeE9xJpiXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# Hyper Parameters \n",
        "input_size = (28, 28)\n",
        "num_classes = 10\n",
        "num_epochs = 10\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1024\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eixF46gUpiXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "c85a845a-167b-4f27-873f-609935a4d800"
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', train=True, \n",
        "                            transform=transforms.Compose([\n",
        "                            transforms.Resize(input_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))]),\n",
        "                            download=True)\n",
        "test_dataset = dsets.MNIST('./data', train=False,transform=transforms.Compose([\n",
        "                            transforms.Resize(input_size),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.1307,), (0.3081,))]), download=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:05, 1812528.46it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 322553.96it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5676680.45it/s]                           \n",
            "8192it [00:00, 128935.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF1n5R8mpiXp",
        "colab_type": "code",
        "outputId": "dbb211d6-b19f-4c72-a0f5-07a9e8abcc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# check the imported dataset shape\n",
        "print(train_dataset[0][0].shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udPohKZzpiXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset loaders (handle mini-batching of data)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True) \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--JjV7hGpiXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_accuracy(loader, total_size, split='train'):\n",
        "    model.eval()\n",
        "    # Measure the accuracy for the entire test dataset \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for sequences, labels in loader:\n",
        "        sequences = sequences.view(-1,28,28)\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(sequences) \n",
        "        _, predicted = torch.max(outputs.data, 1) \n",
        "        total += labels.size(0) \n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print(f'Accuracy of the model on the {total_size} {split} sequences: {float(correct) / total:3.1%}')\n",
        "    \n",
        "    return float(correct) / total    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YbTPwlwpiX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "num_classes = 10\n",
        "\n",
        "# Hyper-parameters\n",
        "n_hidden = 50\n",
        "num_layers = 1 # only works with 1 for now \n",
        "batch_size = batch_size_train\n",
        "learning_rate = 0.001\n",
        "dropout = 0.05\n",
        "\n",
        "# create the internal weight matrix\n",
        "W_res = create_reservoir_matrix(size=(n_hidden, n_hidden), spectral_radius=0.9, sparsity=1.0)\n",
        "\n",
        "model = RNN(input_size, n_hidden, num_layers, num_classes, \n",
        "                  sequence_length=sequence_length, dropout=0.1, return_sequence=True)\n",
        "\n",
        "# set the internal hidden weight matrix as the reservoir values \n",
        "model.hidden.weight_hh_l0 = nn.Parameter(W_res, requires_grad=False)\n",
        "input_scale = 0.5\n",
        "model.hidden.weight_ih_l0 = nn.Parameter((torch.rand((n_hidden,input_size))*2-1)*input_scale, requires_grad=False)\n",
        "\n",
        "# move to the GPU\n",
        "model.to(device)\n",
        "\n",
        "# Freeze all hidden layers so no gradient update occurs\n",
        "for param in model.hidden.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# only need the gradient for the fully connected layer, weight_decay adds l2 norm\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "# CrossEntropyLoss takes care of the Softmax evaluation \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmAoTQ0PpiXX",
        "colab_type": "text"
      },
      "source": [
        "### Train the model \n",
        "Here we train the model and keep track of the train and test losses. Note if you rerun this cell the model will continue to train from it's previous state. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9nha07xpiX3",
        "colab_type": "text"
      },
      "source": [
        "Note this can take around 5 minutes on a CPU, much faster if you run it on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70iJntlhpiX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_acc = []\n",
        "test_acc = []\n",
        "\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (sequences, labels) in enumerate(train_loader):\n",
        "        # reshape to (batch_size, time_step, input size)\n",
        "        sequences = sequences.view(-1,28,28)\n",
        "        sequences = sequences.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad() \n",
        "        outputs = model(sequences) \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # print out your accuracy along the way\n",
        "        if (i + 1) % 70 == 0:\n",
        "            print(f'Epoch: [{epoch+1}/{num_epochs}], Step: [{i+1}/{len(train_dataset)//batch_size}], Loss: {loss.item():3}') \n",
        "    \n",
        "    \n",
        "    # records accuracy on first epoch and on the 5th ones after that\n",
        "    if epoch==0 or (epoch+1)%5==0:\n",
        "        test_acc_epoch = epoch_accuracy(test_loader, len(test_dataset), 'test')\n",
        "        test_acc.append(test_acc_epoch)\n",
        "        train_acc_epoch = epoch_accuracy(train_loader, len(train_dataset), 'train')\n",
        "        train_acc.append(train_acc_epoch)\n",
        "\n",
        "plt.plot(train_acc)\n",
        "plt.plot(test_acc)\n",
        "plt.legend(['train_acc','test_acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iCWPdGNiitw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plFU40N4ijY4",
        "colab_type": "text"
      },
      "source": [
        "Note the difference between training and test accuracy. Some form of regularization should be used to improve the generalization of the model. \n",
        "Examples of regularization to explore include dropout, l1-norm, and l2-norm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ9zhfh0piX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzRH9FbEpiX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdT0Wyw9SPak",
        "colab_type": "text"
      },
      "source": [
        "## Keras Example\n",
        "In this case I also demonstrate an example using the Keras library and in this one I demonstrate the method of running the samples through the reservoir and then classifying using a separate model as a readout layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCAP-0ewpiYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, SimpleRNN, Flatten\n",
        "from keras import initializers\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRbzzE1QpiYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10\n",
        "img_rows, img_cols = 28, 28\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# some data preprocessing and normalizing\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLQhrue-piYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy.random import seed\n",
        "from tensorflow import set_random_seed\n",
        "from keras import regularizers\n",
        "  \n",
        "# for reproducibility\n",
        "set_seed = 0\n",
        "seed(set_seed)\n",
        "set_random_seed(set_seed)\n",
        "\n",
        "n_hidden = 50\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "bias_initializer = 'zeros'\n",
        "dropout = 0.0\n",
        "# this returns all of the reservoir output values when set to True\n",
        "return_sequences = True\n",
        "\n",
        "l1 = 0\n",
        "l2 = 0\n",
        "\n",
        "INPUT_SIZE = (x_train.shape[1], x_train.shape[2])\n",
        "\n",
        "# network is initialized with specific weights and is then only trained\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(n_hidden, activation='tanh', \n",
        "                    kernel_initializer=initializers.RandomUniform(minval=-0.5, maxval=0.5, seed=set_seed),\n",
        "                    bias_initializer=bias_initializer,\n",
        "                    input_shape=INPUT_SIZE,\n",
        "                    return_sequences=return_sequences,\n",
        "                    unroll=False, stateful=False))\n",
        "\n",
        "# In order for the reservoir to have Echo State property the \n",
        "# spectral should be <1\n",
        "spectral_radius = 0.9\n",
        "sparsity = 0.5\n",
        "\n",
        "# gets the weights from the model and generates an effective recurrent weight layer\n",
        "rnn_weights = model.get_weights()\n",
        "esn_recurrent_weights = create_reservoir_matrix(size=rnn_weights[1].shape,\n",
        "                                                  spectral_radius=spectral_radius,\n",
        "                                                  sparsity=sparsity)\n",
        "# assigns the new weights for the recurrent layer to the model and keeps input\n",
        "# weights the same\n",
        "rnn_weights[1] = esn_recurrent_weights\n",
        "model.set_weights(rnn_weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWYrO3P9Uydm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "15c4c35b-9566-4d68-c08e-15dcac3f95fd"
      },
      "source": [
        "\n",
        "# propagate through all the inputs and return final output of cells\n",
        "final_state_train = model.predict(x_train, batch_size=int(len(x_train)/16), verbose=1)\n",
        "final_state_test = model.predict(x_test, batch_size=int(len(x_test)/16), verbose=1)\n",
        "  \n",
        "print(final_state_train.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 1s 13us/step\n",
            "10000/10000 [==============================] - 0s 29us/step\n",
            "(60000, 28, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4RXk2YV2Tq",
        "colab_type": "text"
      },
      "source": [
        "### Final readout layer\n",
        "Here we can use another network to be the readout layer. There are other libraries which can be useful here as well such as with "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3I4PZMVUyv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_final_layer=Sequential()\n",
        "# converts the sequence of average state values to a 1D matrix\n",
        "if return_sequences:\n",
        "  model_final_layer.add(Flatten())\n",
        "model_final_layer.add(Dropout(dropout)) \n",
        "model_final_layer.add(Dense(num_classes, activation='softmax',\n",
        "                            activity_regularizer=regularizers.l1_l2(l1=l1, l2=l2)))\n",
        "                      \n",
        "# set the learning rate and optimizer with some early stopping\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "earlystopper = EarlyStopping(monitor='val_acc', patience=10, verbose=1)\n",
        "\n",
        "model_final_layer.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=adam,\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "history = model_final_layer.fit(final_state_train, y_train, epochs=epochs, \n",
        "                      batch_size=batch_size, \n",
        "                      validation_data=(final_state_test, y_test), verbose=2, \n",
        "                      shuffle=True, callbacks=[earlystopper])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2cDYnXtUy9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot history\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['acc'], label='train accuracy')\n",
        "plt.plot(history.history['val_loss'], label='test loss')\n",
        "plt.plot(history.history['val_acc'], label='test accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}